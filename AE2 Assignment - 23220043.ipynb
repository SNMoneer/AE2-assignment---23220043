{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92b3d4a4",
   "metadata": {},
   "source": [
    "# Assignment Title: Individual report on Vector Embeddings Model & Web Application Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414757f",
   "metadata": {},
   "source": [
    "### Student ID : 23220043"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6fba1",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eedbec",
   "metadata": {},
   "source": [
    "1. Overview of the Assignment:\n",
    "\n",
    "● In this assignment, you are required to collaborate on developing a project\n",
    "with your peers and to write an individual report. The project involves\n",
    "training a vector embeddings model with a web user interface.\n",
    "\n",
    "● Your report should include a detailed description of your involvement in all\n",
    "stages of the software development lifecycle, including planning, analysis,\n",
    "design, implementation and testing. In addition to this, include a “Lessons\n",
    "Learned” chapter where you summarise what you would have done\n",
    "differently if you were working on the project again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2082f",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848764e",
   "metadata": {},
   "source": [
    "2. Data Collection [20 marks]:\n",
    "\n",
    "● Collect a large and diverse textual dataset suitable for training word embeddings. (10 marks)\n",
    "\n",
    "● Recommended sources: Wikipedia dumps, Project Gutenberg, news\n",
    "articles, etc.\n",
    "\n",
    "● Ensure that the dataset is preprocessed: remove special characters,\n",
    "lowercase all words, etc. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc65837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import contractions\n",
    "import unicodedata\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from flask import Flask, request\n",
    "import os\n",
    "import urllib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45893b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link of ebooks from Project Gutenberg\n",
    "a_time_to_die = 'https://www.gutenberg.org/cache/epub/72662/pg72662.txt'\n",
    "alices_adventures_in_wonderland = 'https://www.gutenberg.org/cache/epub/11/pg11.txt'\n",
    "great_expectations = 'https://www.gutenberg.org/cache/epub/1400/pg1400.txt'\n",
    "little_women = 'https://www.gutenberg.org/cache/epub/37106/pg37106.txt'\n",
    "romeo_and_juliet = 'https://www.gutenberg.org/cache/epub/1513/pg1513.txt'\n",
    "the_picture_of_dorian_grey = 'https://www.gutenberg.org/cache/epub/174/pg174.txt'\n",
    "the_scarlet_letter = 'https://www.gutenberg.org/cache/epub/25344/pg25344.txt'\n",
    "the_strange_case_of_drjekyll_and_mrhyde = 'https://www.gutenberg.org/cache/epub/43/pg43.txt'\n",
    "the_yellow_wallpaper = 'https://www.gutenberg.org/cache/epub/1952/pg1952.txt'\n",
    "twenty_years_after = 'https://www.gutenberg.org/cache/epub/1259/pg1259.txt'\n",
    "\n",
    "# List of all the books\n",
    "books = [a_time_to_die, alices_adventures_in_wonderland, great_expectations, little_women, romeo_and_juliet, the_picture_of_dorian_grey, the_scarlet_letter, the_strange_case_of_drjekyll_and_mrhyde, the_yellow_wallpaper, twenty_years_after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a900202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, folder=\".\", filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the ebooks from Project Gutenberg.\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        # Use default file name\n",
    "        filename = os.path.basename(urllib.parse.urlparse(url).path)\n",
    "\n",
    "    # Join folder and filename into a filepath\n",
    "    filepath = os.path.join(folder, filename)\n",
    "\n",
    "    if os.path.isfile(filepath):\n",
    "        print(f'File {filepath} already exists')\n",
    "        return filepath\n",
    "\n",
    "    # Print download message\n",
    "    components = urllib.parse.urlparse(url)\n",
    "    print(f\"Downloading '{os.path.basename(components.path)}' from {components.netloc}\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    except KeyboardInterrupt:\n",
    "        if os.path.exists(filepath):\n",
    "            # Try remove downloaded file\n",
    "            os.remove(filepath)\n",
    "        raise\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    print(f\"Download complete ({dt:.2f}s)\")\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8922d3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File .\\pg72662.txt already exists\n",
      "File .\\pg11.txt already exists\n",
      "File .\\pg1400.txt already exists\n",
      "File .\\pg37106.txt already exists\n",
      "File .\\pg1513.txt already exists\n",
      "File .\\pg174.txt already exists\n",
      "File .\\pg25344.txt already exists\n",
      "File .\\pg43.txt already exists\n",
      "File .\\pg1952.txt already exists\n",
      "File .\\pg1259.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# Create a for loop for all the books in the list to be downloaded as txt files\n",
    "for book in books:\n",
    "    download(book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ef0317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads the txt files\n",
    "\n",
    "# List of all the txt files for the ebooks that have been downloaded\n",
    "txt_files = ['pg72662.txt' , 'pg11.txt' , 'pg1400.txt' , 'pg37106.txt' , 'pg1513.txt' , 'pg174.txt' , 'pg25344.txt' , 'pg43.txt' , 'pg1952.txt' , 'pg1259.txt']\n",
    "\n",
    "# Create a for loop to read all the txt files\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa6eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to preprocess data from the txt files\n",
    "\n",
    "def convert_utf(text):\n",
    "    text = text.replace('\\u2018', \"'\").replace('\\u2019', \"'\").replace('\\u201C', \"`\").replace('\\u201D', \"`\").replace('\\u2013', '-').replace('\\u2014', '-')\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    return text.decode('ascii')\n",
    "\n",
    "def remove_special_characters(words):\n",
    "    # Function to remove non-alphanumeric characters\n",
    "    words = [''.join(char for char in word if char.isalnum()) for word in words]\n",
    "    # Remove empty strings\n",
    "    words = list(filter(None, words))\n",
    "    return words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    # Function to remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def tokenise_sentences(text):\n",
    "    # Convert utf-8 characters to normal characters\n",
    "    text = convert_utf(text)\n",
    "\n",
    "    # Make lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Fix contractions\n",
    "    expanded = contractions.fix(text)\n",
    "\n",
    "    sentences = nltk.sent_tokenize(expanded)\n",
    "\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "\n",
    "        # Remove non-alphanumeric characters\n",
    "        words = remove_special_characters(words)\n",
    "\n",
    "        # Remove stopwords\n",
    "        words = remove_stopwords(words)\n",
    "\n",
    "        data.append(words)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f2184e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocessing function to all the txt files\n",
    "\n",
    "# List of txt files\n",
    "txt_files = ['pg72662.txt' , 'pg11.txt' , 'pg1400.txt' , 'pg37106.txt' , 'pg1513.txt' , 'pg174.txt' , 'pg25344.txt' , 'pg43.txt' , 'pg1952.txt' , 'pg1259.txt']\n",
    "\n",
    "# Iterate through each text file\n",
    "for txt_file in txt_files:\n",
    "    with open(txt_file, 'r', encoding='utf(-8') as file:\n",
    "        # Read content from the file\n",
    "        content = file.read()\n",
    "\n",
    "        # Tokenize sentences using the tokenise_sentences function\n",
    "        tokenized_data = tokenise_sentences(content)\n",
    "\n",
    "        # Tokenized_data contains a list of lists, where each inner list represents the tokenized words of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d3dc3e",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067259fc",
   "metadata": {},
   "source": [
    "3. Training [20 marks]:\n",
    "\n",
    "● Use a Word2Vec embeddings technique. (10 marks)\n",
    "\n",
    "● Utilise Gensim library to assist with the training.\n",
    "\n",
    "● Save the trained model for future use. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bc284f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(vector_size=200, min_count=1, sg=0)\n",
    "model.build_vocab(tokenized_data, update=False)\n",
    "model.train(tokenized_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "model.save('./model_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf654539",
   "metadata": {},
   "source": [
    "4. Web Application [20 marks]:\n",
    "    \n",
    "● Design a simple web interface where a user can input a word. (10 marks)\n",
    "\n",
    "● Implement back-end functionality to fetch the opposite of the given word\n",
    "using the trained embeddings. (10 marks)\n",
    "\n",
    "● Return the opposite word to the user on the web interface.\n",
    "\n",
    "● Use Flask library for the web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64e48f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "model = gensim.models.Word2Vec.load(\"./model_test\")\n",
    "\n",
    "html_form_with_message = f'''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Text Echo App</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h2>Enter Word</h2>\n",
    "    <form method=\"post\" action=\"/\">\n",
    "        <label for=\"text\">Word:</label><br>\n",
    "        <input type=\"text\" name=\"my_input_value\"><br><br>\n",
    "        <input type=\"submit\" value=\"Get Opposite\">\n",
    "    </form>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "def word_negation(word):\n",
    "#     reference_pair = (\"full\", \"empty\")\n",
    "    reference_pair = (\"old\", \"young\")\n",
    "    target_word = word\n",
    "    result_vector = model.wv[target_word] - model.wv[reference_pair[0]] + model.wv[reference_pair[1]]\n",
    "    opposite_words = model.wv.similar_by_vector(result_vector, topn=5)\n",
    "    s = model.wv.most_similar(target_word, topn=5)\n",
    "    print(opposite_words)\n",
    "    return opposite_words\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def home():\n",
    "    if request.method == 'POST':\n",
    "        user_input = request.form['my_input_value']\n",
    "        opposite_word = word_negation(str(user_input))\n",
    "\n",
    "        display_text = \"Input \" + user_input + \"'s opposite word is \" + str(opposite_word)\n",
    "        return f'''\n",
    "            <!DOCTYPE html>\n",
    "            <html>\n",
    "            <head>\n",
    "            <title>Text Echo App</title>\n",
    "            </head>\n",
    "            <body>\n",
    "                <h2>Enter Word</h2>\n",
    "                <form method=\"post\" action=\"/\">\n",
    "                    <label for=\"text\">Word:</label><br>\n",
    "                    <input type=\"text\" name=\"my_input_value\"><br><br>\n",
    "                    <input type=\"submit\" value=\"Get Opposite\">\n",
    "                </form>\n",
    "                <p>{display_text}</p>\n",
    "            </body>\n",
    "            </html>\n",
    "            '''\n",
    "    else:\n",
    "        return html_form_with_message\n",
    "\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d417b",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66eb911",
   "metadata": {},
   "source": [
    "5. Documentation [10 marks]\n",
    "\n",
    "● As part of your report, provide basic UML diagrams capturing the\n",
    "functionality of the application, you can use diagram such as class\n",
    "diagram, component diagram or use case diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ed471",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f4106",
   "metadata": {},
   "source": [
    "6. Report [20 marks]:\n",
    "    \n",
    "● The report should demonstrate involvement in the development process,\n",
    "in all stages:\n",
    "    \n",
    "o Analysis (5 marks)\n",
    "\n",
    "o Design (5 marks)\n",
    "\n",
    "o Implementation (5 marks)\n",
    "\n",
    "o Deployment (5 marks)\n",
    "\n",
    "● The report must include a link to a git repository containing the group\n",
    "project.\n",
    "\n",
    "● The report should be 2500 words (+/- 10%) long plus the code.\n",
    "\n",
    "● The report should be properly formatted, with a title page and appropriate\n",
    "headings and subheadings.\n",
    "\n",
    "● The report should be written in a clear and concise manner, with proper\n",
    "grammar and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1384708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
